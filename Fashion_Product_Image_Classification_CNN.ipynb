{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWdSVrjQYVLm"
      },
      "source": [
        "# **Image Classification using CNN**\n",
        "\n",
        "We want to create a **CNN model** that can identify if an image is related to the following **13 subcategories**:  \n",
        "- **Topwear, Bottomwear, Innerwear, Bags, Watches, Jewellery, Eyewear, Wallets, Shoes, Sandal, Makeup, Fragrance, Others.**  \n",
        "\n",
        "Our **[Kaggle dataset](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small)** contains **44,441 fashion product images**\n",
        "\n",
        "Metadata (images' ID, label, productDisplayName) are in styles.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i9cZudXkdip"
      },
      "source": [
        "## **Step 1: Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xD-4k7xWIns"
      },
      "source": [
        "### **Step 1.1: Get Kaggle API Token**\n",
        "Before downloading from Kaggle, you need to authenticate using the Kaggle API key.\n",
        "\n",
        "1. Go to [Kaggle](https://www.kaggle.com/).\n",
        "2. Click on your **profile picture** (top right).\n",
        "3. Select **Account**.\n",
        "4. Scroll down to **API** → Click **Create New API Token**.\n",
        "5. This will download a file called **kaggle.json**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-93va7nLDrI",
        "outputId": "de093c7d-615f-4926-d850-86e80f7d82a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t\t\t    kaggle\t\t      opt\t\t sys\n",
            "boot\t\t\t    lib\t\t\t      proc\t\t tmp\n",
            "content\t\t\t    lib32\t\t      python-apt\t tools\n",
            "cuda-keyring_1.1-1_all.deb  lib64\t\t      python-apt.tar.xz  usr\n",
            "datalab\t\t\t    libx32\t\t      root\t\t var\n",
            "dev\t\t\t    media\t\t      run\n",
            "etc\t\t\t    mnt\t\t\t      sbin\n",
            "home\t\t\t    NGC-DL-CONTAINER-LICENSE  srv\n"
          ]
        }
      ],
      "source": [
        "!ls /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dCw08HIMBw7",
        "outputId": "bd034730-6430-4c04-a74b-18d0b51c4386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/content/fashion-product-images-small.zip"
      ],
      "metadata": {
        "id": "SZKCitqPo4g2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npC9NyZ7JVA7",
        "outputId": "e26e97be-36f0-415a-880a-878a9836d731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ]
        }
      ],
      "source": [
        "!ls /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "mXIS5rCT2s3O",
        "outputId": "e6d12225-7f7e-433f-85fe-e05c322a5f87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da600ec3-2174-45c8-ada6-71970a621f4d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da600ec3-2174-45c8-ada6-71970a621f4d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ryanmokarian\",\"key\":\"4d82d468f446c7f186e8e9ad2b18aac7\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "# This step only uploads the Kaggle API key and does not download the dataset yet. It simply enables authentication for Kaggle in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5RonqJt4ED3",
        "outputId": "ce374e1c-2e2a-4eff-fe16-4924f31ffbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API key configured successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create a hidden directory for Kaggle API key\n",
        "os.makedirs(\"/root/.kaggle/\", exist_ok=True)\n",
        "\n",
        "# Move the kaggle.json file to this directory\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "\n",
        "# Set permissions to prevent unauthorized access\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "print(\"Kaggle API key configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uh5cgPGYYq2"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O466bQ064QrS",
        "outputId": "198aee68-5b4b-49b9-d88d-213f9c59ef54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\n",
            "License(s): MIT\n",
            "Downloading fashion-product-images-small.zip to /content\n",
            " 99% 560M/565M [00:14<00:00, 37.9MB/s]\n",
            "100% 565M/565M [00:14<00:00, 41.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d paramaggarwal/fashion-product-images-small\n",
        "# the dataset will be downloaded to /content in Colab.\n",
        "# Colab provides a temporary VM, meaning files in /content/ will be lost after a session reset.\n",
        "# To keep your dataset, save it to Google Drive:\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Copy dataset to Google Drive (change the path if needed)\n",
        "# !cp -r /content/content/drive/My\\ Drive/\n",
        "\n",
        "# Later, you can reload it from Google Drive without redownloading from Kaggle:\n",
        "\n",
        "# !cp -r /content/drive/My\\ Drive/content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "idM6JliF4jRe"
      },
      "outputs": [],
      "source": [
        "!unzip fashion-product-images-small.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9CfLF9aeKzQ",
        "outputId": "a317b8cc-bc0f-4a87-f009-13eefeb1cc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-product-images-small.zip  images  myntradataset  sample_data  styles.csv\n"
          ]
        }
      ],
      "source": [
        "# !df -h /content\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1xKvww7br-h"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/train /content/test /content/validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rkPIfHs5BVw",
        "outputId": "40156c4e-7717-4c2c-8bf2-5c8ac63d1ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in dataset folder: ['.config', 'fashion-product-images-small.zip', 'myntradataset', 'styles.csv', 'images', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path where the dataset is extracted\n",
        "dataset_path = \"/content\"\n",
        "\n",
        "# Check files\n",
        "print(\"Files in dataset folder:\", os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9XgAtoJ_LdH"
      },
      "source": [
        "Mount Google Drive in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFZXPbKgA1an",
        "outputId": "20451512-b554-40de-a866-eed84e79effe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin\t\t\t    dev     lib32   NGC-DL-CONTAINER-LICENSE  root  tmp\n",
            "boot\t\t\t    etc     lib64   opt\t\t\t      run   tools\n",
            "content\t\t\t    home    libx32  proc\t\t      sbin  usr\n",
            "cuda-keyring_1.1-1_all.deb  kaggle  media   python-apt\t\t      srv   var\n",
            "datalab\t\t\t    lib     mnt     python-apt.tar.xz\t      sys\n"
          ]
        }
      ],
      "source": [
        "!ls /\n",
        "# !ls /content/drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9XNkZdfGfl0"
      },
      "outputs": [],
      "source": [
        "# print(os.listdir(\"/content/fmyntradataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZMPDefOG4a6"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(\"/content/f/images\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cdz85a3G_9i"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(\"/content/myntradataset/images\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjZzMXGwHgSn",
        "outputId": "dd5abdb8-cdf8-42a5-c7a6-f81b79c5095a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: /content/myntradataset\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the folder you want to delete\n",
        "folder_to_delete = \"/content/myntradataset\"\n",
        "\n",
        "# Delete the folder and its contents\n",
        "shutil.rmtree(folder_to_delete)\n",
        "\n",
        "# Verify deletion\n",
        "print(\"Deleted:\", folder_to_delete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU4v9VXBIYT3"
      },
      "source": [
        "### Step 1.2: Picking a Random Image and Printing Its Shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/fashion-product-images-small.zip"
      ],
      "metadata": {
        "id": "TTC0-mKqxfQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw0fUyq7NYyQ",
        "outputId": "a9a939ec-7c95-4fd9-a9d3-bf2168939c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-product-images-small.zip  images  sample_data  styles.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "irTTYxhHPP6h"
      },
      "outputs": [],
      "source": [
        "#!ls /content/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "6P_wqi_FIu8u",
        "outputId": "84fc1024-0a51-4cef-8f43-5a098b75a598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9d826e89bced>:18: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  pic = imageio.imread(pic_path)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x80>"
            ],
            "image/png": "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\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABQADwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKp6tO9ro97cRcSRQSOv1CkivmgeNvEqoANd1DGM/69qCXKx9R0Zr5eHjvxMP+Y5f/jMalX4g+KlPGt3eM92B/pTsLnR9OUV80r8RvFgx/wATqc891T/4mpF+JXiwNg6vL+KJ/wDE0g50fSVFeVfCzxbrPiDWr621O8a4SO3Ei5VRtO7HYDsa9VoKTuZ2vnHhzVD/ANOkv/oBr5KDqygF1HA719Z+Ijjwzqp/6c5v/QDXymCuxQVB49M00RMiJU/xpnPY04+xU8+tPIiPVFx/u0bIP7i/980EDFPQ5HX1pzc7SDnjOKXybZh9xc/SjyIc8IOfc0wPUvggu7WtVfGCLWMfmx/wr22vF/ghEsepaxtGP3MX/oTV7RUmsdiOaGO4heGZFeORSrqwyGB4INc5/wAK88In/mX7L/viunooKOXPw68IHr4fsv8Avk/40n/CufB+f+QBZ/kf8a6migVkcsfhv4Pb/mX7MfQEf1qJvhj4Nb/mBwj/AHZHH/s1ddRQFkYmh+E9E8NyTSaTZC3aYBZD5jNkDOB8xPqa26KKBn//2Q==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original height, width, and channels of the image: 80 60 3\n"
          ]
        }
      ],
      "source": [
        "import imageio\n",
        "import os\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random.seed(12345)\n",
        "\n",
        "# Define the image folder\n",
        "image_folder = \"/content/images\"\n",
        "\n",
        "# Pick a random image\n",
        "random_pic_file = random.choice(os.listdir(image_folder))\n",
        "pic_path = os.path.join(image_folder, random_pic_file)\n",
        "\n",
        "# Load image\n",
        "pic = imageio.imread(pic_path)\n",
        "\n",
        "# Show image\n",
        "cv2_imshow(pic)\n",
        "\n",
        "# Print shape\n",
        "height, width, channels = pic.shape\n",
        "print(f'Original height, width, and channels of the image: {height} {width} {channels}') # 80 60 3\n",
        "# The image is 80 pixels tall and 60 pixels wide. Standard size is 128x128.\n",
        "# 3 is number of colors (RGB) and it means it is a colored image (not grayscale)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9GS1PW5XW4p"
      },
      "source": [
        "### Step 1.3: Create a dictionary of images-labels\n",
        "- First Convert subCategory to 13 subcategories: Topwear, Bottomwear,\n",
        "Innerwear, Bags, Watches, Jewellery, Eyewear, Wallets, Shoes, Sandal, Makeup,\n",
        "Fragrance, Others.\n",
        "- Then test the label for the random_pic_file (21210.ipg)\n",
        "- Create a dictionary of images and their respective labels  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCSvzL9VaHE9",
        "outputId": "34a1d0ab-bb59-4c02-af9f-25337fcf3707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-product-images-small.zip  images  sample_data  styles.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXG39VyvI6I8"
      },
      "source": [
        "### Step 1.4: Splitting the Dataset into Train, Validation, and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oV8JDZgjOPq"
      },
      "source": [
        "- In case train.csv and test.csv data wants to be uploaded in google drive and mounted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvXuvnBKjbm0"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Step 1: Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Step 2: Define the correct file path based on your folder structure\n",
        "# file_path = \"/content/drive/My Drive/Data_to_mount/Sidhardh/assignment2/test.csv\"\n",
        "\n",
        "# # Step 3: Read the CSV file, assuming tab-separated values\n",
        "# df_teacher_test = pd.read_csv(file_path, delimiter=\"\\t\", header=0, names=[\"imageid\", \"label\", \"productname\"])\n",
        "\n",
        "# # # Step 4: Display the DataFrame\n",
        "# # import ace_tools as tools\n",
        "# # tools.display_dataframe_to_user(name=\"Parsed DataFrame\", dataframe=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njWHoyX9I4qR",
        "outputId": "5ee455ad-0cea-430c-d0a4-41ac0985e53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split complete! Train, validation, and test sets created.\n",
            "Category Mapping: {'Topwear': 0, 'Bottomwear': 1, 'Innerwear': 2, 'Bags': 3, 'Watches': 4, 'Jewellery': 5, 'Eyewear': 6, 'Wallets': 7, 'Shoes': 8, 'Sandal': 9, 'Makeup': 10, 'Fragrance': 11, 'Other': 12}\n",
            "Sample of id_to_label: [('15970', 0), ('39386', 1), ('59263', 4), ('21379', 1), ('53759', 0)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# Define dataset paths\n",
        "dataset_path = \"/content/images\"\n",
        "csv_path = \"/content/styles.csv\"\n",
        "\n",
        "train_path = \"/content/train\"\n",
        "val_path = \"/content/validation\"\n",
        "test_path = \"/content/test\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(val_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "# Load styles.csv and create a label column\n",
        "df = pd.read_csv(csv_path, usecols=[\"id\", \"subCategory\", \"productDisplayName\"])\n",
        "\n",
        "# Define the 12 known categories\n",
        "known_categories = [\n",
        "    \"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\",\n",
        "    \"Jewellery\", \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\",\n",
        "    \"Makeup\", \"Fragrance\"\n",
        "]\n",
        "\n",
        "# Step 1: Map all subcategories outside of the 12 categories to \"Other\"\n",
        "df[\"label\"] = df[\"subCategory\"].apply(lambda x: x if x in known_categories else \"Other\")\n",
        "\n",
        "# Step 2: Create a mapping dictionary (category to integer)\n",
        "category_mapping = {category: idx for idx, category in enumerate(known_categories + [\"Other\"])}\n",
        "\n",
        "# Step 3: Convert labels to numerical values\n",
        "df[\"label\"] = df[\"label\"].map(category_mapping)\n",
        "\n",
        "# Create an id-to-label dictionary\n",
        "id_to_label = dict(zip(df[\"id\"].astype(str), df[\"label\"]))\n",
        "\n",
        "# Get list of image files\n",
        "all_images = os.listdir(dataset_path)\n",
        "\n",
        "# Split into train (80%), validation (10%), test (10%)\n",
        "train_files, test_val_files = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "val_files, test_files = train_test_split(test_val_files, test_size=0.5, random_state=42)\n",
        "\n",
        "# Copy files to respective directories\n",
        "for file in train_files:\n",
        "    shutil.copy(os.path.join(dataset_path, file), os.path.join(train_path, file))\n",
        "\n",
        "for file in val_files:\n",
        "    shutil.copy(os.path.join(dataset_path, file), os.path.join(val_path, file))\n",
        "\n",
        "for file in test_files:\n",
        "    shutil.copy(os.path.join(dataset_path, file), os.path.join(test_path, file))\n",
        "\n",
        "print(\"Dataset split complete! Train, validation, and test sets created.\")\n",
        "print(\"Category Mapping:\", category_mapping)\n",
        "print(\"Sample of id_to_label:\", list(id_to_label.items())[:5])  # Show sample mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GN3LrCnJqqN_",
        "outputId": "79937299-20f9-48f5-e4ad-aaac9f8df8fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id subCategory                             productDisplayName  label\n",
              "0  15970     Topwear               Turtle Check Men Navy Blue Shirt      0\n",
              "1  39386  Bottomwear             Peter England Men Party Blue Jeans      1\n",
              "2  59263     Watches                       Titan Women Silver Watch      4\n",
              "3  21379  Bottomwear  Manchester United Men Solid Black Track Pants      1\n",
              "4  53759     Topwear                          Puma Men Grey T-shirt      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c12d7546-e84a-41d9-a001-e316374bf3c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>subCategory</th>\n",
              "      <th>productDisplayName</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15970</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39386</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Peter England Men Party Blue Jeans</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59263</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Titan Women Silver Watch</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21379</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Manchester United Men Solid Black Track Pants</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53759</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Puma Men Grey T-shirt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c12d7546-e84a-41d9-a001-e316374bf3c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c12d7546-e84a-41d9-a001-e316374bf3c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c12d7546-e84a-41d9-a001-e316374bf3c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-785048ef-31ff-4916-940e-fba758f402c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-785048ef-31ff-4916-940e-fba758f402c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-785048ef-31ff-4916-940e-fba758f402c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 44446,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17048,\n        \"min\": 1163,\n        \"max\": 60000,\n        \"num_unique_values\": 44446,\n        \"samples\": [\n          40435,\n          18612,\n          21998\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subCategory\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          \"Hair\",\n          \"Makeup\",\n          \"Free Gifts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"productDisplayName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31135,\n        \"samples\": [\n          \"Nike Women White Air Dictate MSL Sports Shoes\",\n          \"Ivory Tag Women Rock Array Blue and Red Jewellery Set\",\n          \"Femella Women Pink T-shirt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 12,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          7,\n          5,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-GPftqpq1HS",
        "outputId": "6cfbd5eb-2beb-40f4-fd6f-a68aff9504a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44446, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_Pkq3huypkS",
        "outputId": "b597b305-ec36-446c-8834-79543da81ceb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# label_dict[\"29114\"]\n",
        "id_to_label[\"29114\"] # 29114 subCategory is Socks -> label: Other -> label id: 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILAVFak6RP-p",
        "outputId": "3d411992-086b-43d2-8776-230488ad5e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fashion-product-images-small.zip  images  sample_data  styles.csv  test  train\tvalidation\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMnWaqwyKbwK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# print(os.listdir(\"/content\")) # ['train', 'styles.csv', 'images', 'validation', 'test']\n",
        "print(os.listdir(\"/content/test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8clTmpK0Kf",
        "outputId": "42cea5bc-f37a-46f1-ebaf-b41fa95308bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4445\n"
          ]
        }
      ],
      "source": [
        "test_path = \"/content/test\"\n",
        "print(len(os.listdir(test_path))) # 4445"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do1zZqnSJmpK"
      },
      "source": [
        "### Step 1.5: Count the Number of Images in Each Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiwQF62mJrOT",
        "outputId": "1bba584d-e8d2-44e1-9dcc-3cede584dc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 35552 images in the folder at /content/train\n",
            "There are 4444 images in the folder at /content/validation\n",
            "There are 4445 images in the folder at /content/test\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "# Define dataset directories\n",
        "folder_path_options = [train_path, val_path, test_path]\n",
        "\n",
        "for path in folder_path_options:\n",
        "    files = glob.glob(path + \"/*\")\n",
        "    file_count = len(files)\n",
        "    print(f'There are {file_count} images in the folder at {path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EdoTFejXsm"
      },
      "source": [
        "# **Step 2: Data Loader for Fashion Image Dataset**\n",
        "In this step, we will:\n",
        "1. **Define preprocessing functions** (image transformations).\n",
        "2. **Create a custom PyTorch dataset class** to load images.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Do We Need a Data Loader?**\n",
        "- **Efficient Data Loading:** PyTorch requires an efficient way to read images in batches during training.\n",
        "- **Image Preprocessing:** Resize images, normalize, and apply transformations (e.g., flipping, cropping).\n",
        "- **Memory Optimization:** Loading images dynamically instead of keeping everything in memory.\n",
        "- **Integration with Neural Network:** The DataLoader will provide images **to the CNN model** during training and inference.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxwII3MMk55A"
      },
      "source": [
        "### **Step 2.1: Define Image Preprocessing Pipeline**\n",
        "We will use **torchvision.transforms** to:\n",
        "- **Resize** images to `(128, 128)`.\n",
        "- **Convert them into tensors** for PyTorch compatibility.\n",
        "- **Normalize the pixel values** to improve convergence.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqIDQJRZjlmz"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
        "    transforms.RandomHorizontalFlip(),  # Augment data with horizontal flips\n",
        "    transforms.ToTensor(),  # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Validation and test transformations (no augmentation)\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPNoNxDyj8Ne"
      },
      "source": [
        "Why Normalize?\n",
        "\n",
        "Normalization helps improve model training by making pixel values range between -1 to 1 instead of 0 to 255.\n",
        "This makes the model converge faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrtE3AsGjrMG"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 2.2: Create a Custom PyTorch Dataset Class**\n",
        "We now define a dataset class to **load images dynamically**.\n",
        "\n",
        "✅ **Why Do We Need This Custom Dataset?**\n",
        "- **Handles image loading dynamically** without keeping everything in memory.\n",
        "- **Applies transformations automatically** when an image is loaded.\n",
        "- **Extracts labels from filenames**, assuming files are named like `\"5_abc123.jpg\"` (where `5` is the class label).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyTCnuKZj2KE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define the FashionDataset class\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, path, label_dict, transform=None):\n",
        "        self.path = path\n",
        "        self.label_dict = label_dict  # Store id-label mapping\n",
        "        self.transform = transform\n",
        "        self.files = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".jpg\")])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.files[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Convert image to RGB\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract label from dictionary using image id\n",
        "        img_id = int(os.path.basename(img_path).split(\".\")[0])  # Convert filename to integer id\n",
        "        label = self.label_dict.get(str(img_id), 12)  # Default to \"Other\" (index 12) if not found\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)  # Convert label to integer tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QCUVUJtkQy2"
      },
      "source": [
        "---\n",
        "\n",
        "### **Step 2.3: Create Datasets and DataLoaders**\n",
        "We now **load the images from train, validation, and test folders**.\n",
        "\n",
        "✅ **What Are DataLoaders?**\n",
        "- **Efficient batch processing:** Instead of loading all images at once, DataLoader loads a **batch of 32 images at a time**.\n",
        "- **Shuffling for randomness:** The training set is shuffled to **prevent the model from memorizing patterns**.\n",
        "- **Faster Training:** Handles parallel processing of images.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NGWt5BGkUaj",
        "outputId": "fe313b62-9022-47e1-d768-42e2504efdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Loaders Created Successfully!\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define dataset paths\n",
        "train_path = \"/content/train\"\n",
        "val_path = \"/content/validation\"\n",
        "test_path = \"/content/test\"\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = FashionDataset(train_path, label_dict=id_to_label, transform=train_transforms)\n",
        "val_dataset = FashionDataset(val_path, label_dict=id_to_label, transform=val_transforms)\n",
        "test_dataset = FashionDataset(test_path, label_dict=id_to_label, transform=test_transforms)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(\"✅ Data Loaders Created Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kIV_V4wn_N6"
      },
      "source": [
        "# Step 3: Define a Neural Network Model for Fashion Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U04CWajqoB5z"
      },
      "source": [
        "Now that we have prepared the DataLoader, we will define a Convolutional Neural Network (CNN) to classify images into 13 fashion categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BKvgQ_9oEqM"
      },
      "source": [
        "Why Use a CNN for Image Classification?\n",
        "CNNs are designed to process spatial data (like images).\n",
        "They use convolutional layers to detect patterns such as edges, textures, and shapes.\n",
        "Pooling layers reduce dimensionality, making the model more efficient.\n",
        "The final fully connected layers classify the image based on extracted features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOF6DCPnoI3q"
      },
      "source": [
        "### Step 3.1: Define the CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXNgCCNPoM74"
      },
      "source": [
        "We will build a CNN with multiple convolutional layers, followed by fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhpPTZbcoP0k",
        "outputId": "d79d20b1-f173-4029-f7f6-295e8170c3c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FashionCNN(\n",
            "  (cnn): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=65536, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=13, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the CNN Model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=13):\n",
        "        super(FashionCNN, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * 16 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = FashionCNN(num_classes=13)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knb0ihyeoTnV"
      },
      "source": [
        "### Step 3.2: Model Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0oBoZOoYz-"
      },
      "source": [
        "Convolutional Layers (self.cnn)\n",
        "\n",
        "Detect edges, textures, and shapes.\n",
        "Uses Batch Normalization for stable learning.\n",
        "ReLU Activation introduces non-linearity.\n",
        "MaxPooling reduces feature map size.\n",
        "Fully Connected Layers (self.fc)\n",
        "\n",
        "Flattens the CNN output.\n",
        "Uses Dense Layers to classify the features.\n",
        "The final layer outputs 13 class predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhr4KYuEoasp"
      },
      "source": [
        "### Step 3.3: Verify the Model Output Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvhglALgodUd"
      },
      "source": [
        "To check if the model works properly, pass a random tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f9kqav-ofHW",
        "outputId": "d2d48d62-27fd-4aa6-b5b8-c4c71f53865c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Output Shape: torch.Size([1, 13])\n",
            "🔹 Raw Tensor Values for 13 Categories: tensor([[ 0.1157, -0.2244,  0.4162, -0.0359,  0.2067,  0.2025, -0.0418,  0.2917,\n",
            "         -0.0218,  1.1571,  0.1798,  0.1572, -0.1204]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "🔹 Probabilities: tensor([[0.0678, 0.0482, 0.0915, 0.0582, 0.0742, 0.0739, 0.0579, 0.0808, 0.0591,\n",
            "         0.1920, 0.0722, 0.0706, 0.0535]], grad_fn=<SoftmaxBackward0>)\n",
            "🔹 Predicted Class Index: 9\n",
            "🔹 Predicted Class Label: Sandal\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a random tensor with batch size 1, 3 color channels, and 128x128 image size\n",
        "sample_input = torch.randn(1, 3, 128, 128)\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(sample_input)  # Raw logits before applying softmax\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Get predicted class index\n",
        "predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "# Get the predicted class label from the 13 categories\n",
        "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\",\n",
        "                \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Other\"]\n",
        "\n",
        "predicted_class_label = class_labels[predicted_class_idx]\n",
        "\n",
        "# Print all required information\n",
        "print(\"🔹 Output Shape:\", output.shape)  # Expected: torch.Size([1, 13])\n",
        "print(\"🔹 Raw Tensor Values for 13 Categories:\", output)  # Logits before softmax\n",
        "print(\"🔹 Probabilities:\", probabilities)  # Softmax probabilities\n",
        "print(\"🔹 Predicted Class Index:\", predicted_class_idx)  # Index with the highest probability\n",
        "print(\"🔹 Predicted Class Label:\", predicted_class_label)  # Mapped category name\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Cys0NR09jd"
      },
      "source": [
        "# Step 4: Initialize the Train/Validation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHPSw7Jp1L7K"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set batch size and number of epochs\n",
        "batch_size = 32\n",
        "n_epochs = 4\n",
        "patience = 10     # Early stopping patience (prevents overfitting)\n",
        "\n",
        "# Move the model to the correct device\n",
        "model = FashionCNN(num_classes=13).to(device)\n",
        "\n",
        "# Define loss function (Cross-Entropy Loss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (Adam optimizer with learning rate and weight decay)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "\n",
        "# Create Data Loaders (Assumes train_dataset & val_dataset are already defined)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Trackers for early stopping\n",
        "best_acc = 0\n",
        "stale = 0  # Counter for early stopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QXdF6hf1Nsy"
      },
      "source": [
        "# Step 5: Train and Validate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cWH4J-s1S1h",
        "outputId": "8901f723-acaa-4043-adb5-343eca5d0c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Progress: 100%|██████████| 1111/1111 [1:47:23<00:00,  5.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4: Train Loss: 0.6731, Train Acc: 0.8101\n",
            "Epoch 1/4:\n",
            "Train Loss: 0.6731, Train Acc: 0.8101\n",
            "Val Loss: 0.3446, Val Acc: 0.8884\n",
            "Epoch 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Progress: 100%|██████████| 1111/1111 [1:50:43<00:00,  5.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/4: Train Loss: 0.3839, Train Acc: 0.8826\n",
            "Epoch 2/4:\n",
            "Train Loss: 0.3839, Train Acc: 0.8826\n",
            "Val Loss: 0.2957, Val Acc: 0.9113\n",
            "Epoch 3/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Progress: 100%|██████████| 1111/1111 [1:51:56<00:00,  6.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/4: Train Loss: 0.3208, Train Acc: 0.9006\n",
            "Epoch 3/4:\n",
            "Train Loss: 0.3208, Train Acc: 0.9006\n",
            "Val Loss: 0.2569, Val Acc: 0.9251\n",
            "Epoch 4/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Progress: 100%|██████████| 1111/1111 [1:49:44<00:00,  5.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4: Train Loss: 0.2801, Train Acc: 0.9142\n",
            "Epoch 4/4:\n",
            "Train Loss: 0.2801, Train Acc: 0.9142\n",
            "Val Loss: 0.2270, Val Acc: 0.9320\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss, train_correct = 0, 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Progress\"):\n",
        "        imgs, labels = batch  # Unpack images and labels\n",
        "        imgs = imgs.to(device)  # Move images to device\n",
        "        labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)  # Forward pass\n",
        "        loss = criterion(logits, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss, val_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            imgs, labels = batch\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / len(val_dataset)\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        stale = 0  # Reset stale counter\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
        "    else:\n",
        "        stale += 1\n",
        "\n",
        "    if stale >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break  # Stop training if no improvement\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuAQ3Gjl88at"
      },
      "source": [
        "## Save the Model After Training using torch.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plyKPJ7KDVVo",
        "outputId": "2a60d4cf-3143-4da5-f99e-f5cc0700ff78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiC5Hxlx9I-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19501ae-1c5e-4f76-e752-a4f93ad6c0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Model saved to Google Drive at /content/drive/My Drive/fashion_cnn.pth\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "\n",
        "# # Define the model save path\n",
        "# model_save_path = \"/content/fashion_cnn.pth\"\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# If you're using Google Drive, you can save it there for persistent storage:\n",
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "#  Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#  Define save path inside Google Drive\n",
        "model_save_path = \"/content/drive/My Drive/fashion_cnn.pth\"\n",
        "# fashion_cnn.pth will contain the parameters (weights & biases) of your trained CNN model\n",
        "\n",
        "#  Save trained model weights\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\" Model saved to Google Drive at {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPXDWQhk9aHx"
      },
      "source": [
        "## Reload the Model in a New Google Colab Session\n",
        "(A) What to Do After Reopening Google Colab\n",
        "After reopening Colab, you need to:\n",
        "\n",
        "- Re-upload the dataset (or re-download it from Kaggle)\n",
        "- Recreate train, validation, and test datasets\n",
        "- Reinitialize the CNN model architecture\n",
        "- Load the saved model weights instead of training again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmNPthLn9tED"
      },
      "source": [
        "(B) Load the Model Without Training Again\n",
        "-  Once the datasets are ready, you can load the model instead of retraining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDEkuYuO9xl-"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# # Define the same model architecture (must match the saved model)\n",
        "# model = FashionCNN(num_classes=13)\n",
        "\n",
        "# # Load the trained weights\n",
        "# model_load_path = \"/content/fashion_cnn.pth\"  # Update path if saved in Drive\n",
        "# model.load_state_dict(torch.load(model_load_path))\n",
        "\n",
        "# # Set model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# print(\"Model loaded successfully and ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6haBp9o90nP"
      },
      "source": [
        "If you saved the model in Google Drive, first mount Drive and update the path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eekGBgmw92N9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc853a6-1dc9-4adc-a740-eb1bffbf50a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-fa887fbca9bb>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully and ready for inference!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "# Mount Google Drive again\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the same model architecture\n",
        "model = FashionCNN(num_classes=13)  # ⚠️ Must match the original model definition\n",
        "\n",
        "# Load the trained weights\n",
        "model_load_path = \"/content/drive/My Drive/fashion_cnn.pth\"\n",
        "model.load_state_dict(torch.load(model_load_path))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully and ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6TIekT98Fn"
      },
      "source": [
        "- Since the model is loaded and set to evaluation mode (model.eval()), you DO NOT need to run Step 5 (training).\n",
        "- You CAN use the model for predictions on new images directly.\n",
        "- Simply load test images and run them through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evm1ofkv9dgS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OshZvw5p1U0y"
      },
      "source": [
        "# Step 6: Apply the Model on Test Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiesArVI1ZQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f1eeb8-7887-4072-f5f0-ded4e593f7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Evaluating test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Processing Batches: 100%|██████████| 35/35 [05:15<00:00,  9.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test Accuracy: 0.9415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Move model to the correct device (if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Ensure test dataset exists before proceeding\n",
        "if 'test_dataset' not in globals():\n",
        "    raise ValueError(\"❌ Error: test_dataset is not defined!\")\n",
        "\n",
        "# Create DataLoader for test dataset\n",
        "batch_size = 128  # Larger batch size speeds up evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Evaluate on test dataset\n",
        "test_correct = 0\n",
        "total_images = len(test_dataset)\n",
        "\n",
        "print(\"🔄 Evaluating test dataset...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc=\"🔍 Processing Batches\"):  # tqdm added\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        test_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
        "\n",
        "# Calculate & print accuracy\n",
        "test_acc = test_correct / total_images\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rU10mET-W3s"
      },
      "source": [
        "# Step 7: Run Predictions on New Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrONWBKB-cMs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "f433e0a8-dcde-4efb-f24a-6b156bd714e9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Predicted Label: Topwear (Index: 0)\n",
            "🖼️ Image Path: /content/test/32392.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define image transformations (must match the ones used during training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Select a random image from test folder\n",
        "test_image_folder = \"/content/test\"\n",
        "test_images = os.listdir(test_image_folder)\n",
        "\n",
        "if not test_images:\n",
        "    raise FileNotFoundError(\"❌ Error: No images found in test dataset!\")\n",
        "\n",
        "# Pick a random test image\n",
        "test_image_path = os.path.join(test_image_folder, random.choice(test_images))\n",
        "\n",
        "# Load and preprocess the image\n",
        "image = Image.open(test_image_path).convert(\"RGB\")\n",
        "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Display the selected image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title(\"Selected Test Image\")\n",
        "plt.show()\n",
        "\n",
        "# Perform prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor.to(device))\n",
        "    predicted_label_idx = output.argmax(dim=1).item()\n",
        "\n",
        "# Define category labels (ensure this matches the training labels)\n",
        "class_labels = [\"Topwear\", \"Bottomwear\", \"Innerwear\", \"Bags\", \"Watches\", \"Jewellery\",\n",
        "                \"Eyewear\", \"Wallets\", \"Shoes\", \"Sandal\", \"Makeup\", \"Fragrance\", \"Other\"]\n",
        "\n",
        "# Get the predicted category name\n",
        "predicted_label = class_labels[predicted_label_idx]\n",
        "\n",
        "# Print results\n",
        "print(f\"✅ Predicted Label: {predicted_label} (Index: {predicted_label_idx})\")\n",
        "print(f\"🖼️ Image Path: {test_image_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCY0Wot8N7YH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}